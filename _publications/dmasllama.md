---
title: "Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach"
collection: publications
permalink: /publication/dmasllama
excerpt: 'This paper is about our study on training a 1.7B LLaMa model'
date: 2025-06-30
venue: 'IJCNN 2025'
paperurl: 'https://arxiv.org/abs/2412.13335'
citation: 'Miles Q. Li, Benjamin CM Fung, and Shih-Chia Huang. Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach. In Proceedings of the International Conference on Joint Conference on Neural Networks (IJCNN), pages 1-10, Roma, Italy: IEEE, June 2025.'
---
Deep learning models have achieved state-of-the-art performance in many classification tasks. However, most of them cannot provide an explanation for their classification results. Machine learning models that are interpretable are usually linear or piecewise linear and yield inferior performance. Non-linear models achieve much better classification performance, but it is hard to explain their classification results. This may have been changed by an interpretable feedforward neural network (IFFNN) proposed that achieves both high classification performance and interpretability for malware detection. If the IFFNN can perform well in a more flexible and general form for other classification tasks while providing meaningful explanations, it may be of great interest to the applied machine learning community. In this paper, we propose a way to generalize the interpretable feedforward neural network to multi-class classification scenarios and any type of feedforward neural networks, and evaluate its classification performance and interpretability on intrinsic interpretable datasets. We conclude by finding that the generalized IFFNNs achieve comparable classification performance to their normal feedforward neural network counterparts and provide meaningful explanations. Thus, this kind of neural network architecture has great practical use.

[Download paper here](https://arxiv.org/abs/2412.13335)

Recommended citation: Miles Q. Li, Benjamin CM Fung, and Shih-Chia Huang. Training Dynamics of a 1.7B LLaMa Model: A Data-Efficient Approach. In Proceedings of the International Conference on Joint Conference on Neural Networks (IJCNN), pages 1-10, Roma, Italy: IEEE, June 2025.